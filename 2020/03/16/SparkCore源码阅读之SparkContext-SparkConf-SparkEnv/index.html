<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thisiswilli.cn","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="SparkCore之SparkContext,SparkConf本文记录SparkCore,SparkContext,SparkConf源码阅读笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkCore源码阅读之SparkContext,SparkConf,SparkEnv">
<meta property="og:url" content="https://thisiswilli.cn/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/index.html">
<meta property="og:site_name" content="ThisisWilli">
<meta property="og:description" content="SparkCore之SparkContext,SparkConf本文记录SparkCore,SparkContext,SparkConf源码阅读笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://willipic.oss-cn-hangzhou.aliyuncs.com/Spark/SparkEnv%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="https://willipic.oss-cn-hangzhou.aliyuncs.com/concurrency/SparkContext%E6%B5%81%E7%A8%8B.001.png">
<meta property="article:published_time" content="2020-03-16T14:40:57.000Z">
<meta property="article:modified_time" content="2020-07-10T07:20:21.229Z">
<meta property="article:author" content="Willi">
<meta property="article:tag" content="big data">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://willipic.oss-cn-hangzhou.aliyuncs.com/Spark/SparkEnv%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B.png">

<link rel="canonical" href="https://thisiswilli.cn/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>SparkCore源码阅读之SparkContext,SparkConf,SparkEnv | ThisisWilli</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0dc46e5d0914c07dc8b23b28c12d8fb3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="ThisisWilli" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ThisisWilli</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Time is now</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">17</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thisiswilli.cn/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/xiaolang.jpeg">
      <meta itemprop="name" content="Willi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ThisisWilli">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkCore源码阅读之SparkContext,SparkConf,SparkEnv
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-16 22:40:57" itemprop="dateCreated datePublished" datetime="2020-03-16T22:40:57+08:00">2020-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-10 15:20:21" itemprop="dateModified" datetime="2020-07-10T15:20:21+08:00">2020-07-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="SparkCore之SparkContext-SparkConf"><a href="#SparkCore之SparkContext-SparkConf" class="headerlink" title="SparkCore之SparkContext,SparkConf"></a>SparkCore之SparkContext,SparkConf</h1><p>本文记录SparkCore,SparkContext,SparkConf源码阅读笔记</p>
<a id="more"></a>
<h2 id="Spark程序入口"><a href="#Spark程序入口" class="headerlink" title="Spark程序入口"></a>Spark程序入口</h2><p>​        我们在编写Spark程序时，程序的入口一般都需创建SparkConf，设置SparkConf中的属性，再将SparkConf传入SparkContext中，比如一个简单的map算子程序，接下来通过SparkConf，SparkContext的源码，来解读Spark程序初始化的过程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">map</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"map"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"Error"</span>)</span><br><span class="line">    <span class="comment">// 读取数据</span></span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(<span class="string">"data/data.txt"</span>)</span><br><span class="line">    lines.map(one=&gt;&#123;</span><br><span class="line">      one + <span class="string">"#"</span></span><br><span class="line">    &#125;).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SparkConf"><a href="#SparkConf" class="headerlink" title="SparkConf"></a>SparkConf</h2><p>SparkConf是Spark application的配置类，SparkConf主要有以下几点功能以及特点：</p>
<ul>
<li><p>在读取SparkConf参数时，SparkConf会先读取org.apache.spark下任何关于SparkConf的配置，是否读取读取默认参数可以在创建SparkConf时通过传入true或false进行设置，但是开发者通过set方法设置的SparkConf属性的优先级是高于系统设置的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">loadFromSystemProperties</span></span>(silent: <span class="type">Boolean</span>): <span class="type">SparkConf</span> = &#123;</span><br><span class="line">    <span class="comment">// Load any spark.* system properties</span></span><br><span class="line">    <span class="keyword">for</span> ((key, value) &lt;- <span class="type">Utils</span>.getSystemProperties <span class="keyword">if</span> key.startsWith(<span class="string">"spark."</span>)) &#123;</span><br><span class="line">      set(key, value, silent)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>开发者可以在SparkConf中set SparkConf中的各种参数，其中set方法都支持链式书写，如</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"map"</span>)</span><br></pre></td></tr></table></figure>

<p>开发者也可以通过get获取SparkConf中的参数，部分源码如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The master URL to connect to, such as "local" to run locally with one thread, "local[4]" to</span></span><br><span class="line"><span class="comment">   * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setMaster</span></span>(master: <span class="type">String</span>): <span class="type">SparkConf</span> = &#123;</span><br><span class="line">    set(<span class="string">"spark.master"</span>, master)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Set a name for your application. Shown in the Spark web UI. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setAppName</span></span>(name: <span class="type">String</span>): <span class="type">SparkConf</span> = &#123;</span><br><span class="line">    set(<span class="string">"spark.app.name"</span>, name)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/** Get a parameter; throws a NoSuchElementException if it's not set */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(key: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    getOption(key).getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span>(key))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Get a parameter, falling back to a default if not set */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(key: <span class="type">String</span>, defaultValue: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    getOption(key).getOrElse(defaultValue)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>SparkConf也定义了一个伴生对象来记录已经弃用的配置</p>
</li>
</ul>
<h2 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h2><p>SparkEnv是Spark运行程序的运行环境，SparkEnv中保留正在运行的Spark实例（主实例或工作实例）的所有运行时环境对象，SparkEnv类的构造函数如下，可见这个类中定义了Spark application运行时的环境对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkEnv</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val executorId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private[spark] val rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val closureSerializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializerManager: <span class="type">SerializerManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val mapOutputTracker: <span class="type">MapOutputTracker</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val shuffleManager: <span class="type">ShuffleManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val broadcastManager: <span class="type">BroadcastManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val blockManager: <span class="type">BlockManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val securityManager: <span class="type">SecurityManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val metricsSystem: <span class="type">MetricsSystem</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val memoryManager: <span class="type">MemoryManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val outputCommitCoordinator: <span class="type">OutputCommitCoordinator</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">var</span> isStopped = <span class="literal">false</span></span><br><span class="line">  ....&#125;</span><br></pre></td></tr></table></figure>

<p>SparkContext在执行Spark application时会根据SparkConf中的设置，通过阅读SparkEnv源码中的create方法可知，SparkEnv中的运行时环境对象创建顺序如下所示，接下来我们逐个分析</p>
<img src="https://willipic.oss-cn-hangzhou.aliyuncs.com/Spark/SparkEnv%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B.png" style="zoom:25%;" />

<h3 id="安全管理器SecurityManager"><a href="#安全管理器SecurityManager" class="headerlink" title="安全管理器SecurityManager"></a>安全管理器SecurityManager</h3><p>create方法中创建SecurityManager的代码如下所示，创建SecurityManager需要SparkConf中的设置以及io加密的密钥</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> securityManager = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf, ioEncryptionKey)</span><br><span class="line">    <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      securityManager.initializeAuth()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ioEncryptionKey.foreach &#123; _ =&gt;</span><br><span class="line">      <span class="keyword">if</span> (!securityManager.isEncryptionEnabled()) &#123;</span><br><span class="line">        logWarning(<span class="string">"I/O encryption enabled without RPC encryption: keys will be visible on the "</span> +<span class="string">"wire."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>再看SecurityManager这个类，顾名思义，主要负责安全，主要有以下几个特点</p>
<ul>
<li><p>SecurityManager可以设置集群中的通信方式，Spark集群之间默认可以通过公钥来进行验证并通信，可以配置“ spark.authenticate”将身份验证设置为启用，进一步提高安全性</p>
</li>
<li><p>也可以通过javax servlet过滤器让对特定数据没有访问权限的用户无法访问相应的SparkUI</p>
</li>
<li><p>Spark通过个人和小组修改ACL（<code>spark.modify.acls</code>）和（<code>spark.modify.acls.groups</code>）控制哪些用户和组织有权访问修改单个Spark application</p>
</li>
<li><p>如果使用Hadoop YARN作为集群管理器，则需要使用证书生成secret key登录，最后给当前系统设置默认的口令认证实例</p>
</li>
</ul>
<h3 id="RPC通信环境RpcEnv"><a href="#RPC通信环境RpcEnv" class="headerlink" title="RPC通信环境RpcEnv"></a>RPC通信环境RpcEnv</h3><p>create中创建RPCEnv的代码如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, bindAddress, advertiseAddress, port.getOrElse(<span class="number">-1</span>), conf,</span><br><span class="line">      securityManager, numUsableCores, !isDriver)</span><br></pre></td></tr></table></figure>

<p>关于RPC的分析可以参考<a href="https://zhuanlan.zhihu.com/p/28893155" target="_blank" rel="noopener">这篇文章</a></p>
<h3 id="序列化管理器serializerManager"><a href="#序列化管理器serializerManager" class="headerlink" title="序列化管理器serializerManager"></a>序列化管理器serializerManager</h3><p>​        序列化管理器通过用来配置各Spark组件的序列化、压缩和加密。serializer默认实现为org.apache.spark.serializer.JavaSerializer，用户可以通过在SparkConf中自定义序列化器的类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> serializer = instantiateClassFromConf[<span class="type">Serializer</span>](</span><br><span class="line">      <span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.JavaSerializer"</span>)</span><br><span class="line">    logDebug(<span class="string">s"Using serializer: <span class="subst">$&#123;serializer.getClass&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> serializerManager = <span class="keyword">new</span> <span class="type">SerializerManager</span>(serializer, conf, ioEncryptionKey)</span><br></pre></td></tr></table></figure>

<h3 id="闭包序列化器closureSerializer"><a href="#闭包序列化器closureSerializer" class="headerlink" title="闭包序列化器closureSerializer"></a>闭包序列化器closureSerializer</h3><p>闭包序列化器只能为java内置序列化器，不支持用户自定义</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> closureSerializer = <span class="keyword">new</span> <span class="type">JavaSerializer</span>(conf)</span><br></pre></td></tr></table></figure>

<h3 id="广播管理器broadcastManager"><a href="#广播管理器broadcastManager" class="headerlink" title="广播管理器broadcastManager"></a>广播管理器broadcastManager</h3><p>​        create方法中创建broadcastManager代码如下所示，BroadcastManager用于将配置信息和序列化后的RDD、Job以及ShuffleDependency等信息在本地存储。如果为了容灾，也会复制到其他节点上</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcastManager = <span class="keyword">new</span> <span class="type">BroadcastManager</span>(isDriver, conf, securityManager)</span><br></pre></td></tr></table></figure>

<p>###Map任务输出跟踪器mapOutputTracker </p>
<p>MapOutputTracker 用于跟踪 stage中map任务输出结果的位置</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mapOutputTracker = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">     <span class="keyword">new</span> <span class="type">MapOutputTrackerMaster</span>(conf, broadcastManager, isLocal)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">new</span> <span class="type">MapOutputTrackerWorker</span>(conf)</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>MapOutputTrackerMaster属于Driver，负责追踪当前Spark application上所有Shuffle的Map输出元数据信息</li>
<li>MapOutputTrackerWorker是运行在Executor中的，负责拉取MapOutputTrackerMaster端的map输出数据，进行reduce任务</li>
</ul>
<h3 id="ShuffleManager"><a href="#ShuffleManager" class="headerlink" title="ShuffleManager"></a>ShuffleManager</h3><p>​        Spark的stage与stage之间的过程就是 shuffle 阶段，在 Spark 中，负责 shuffle 过程的执行、计算和处理的组件主要就是 <strong>ShuffleManager</strong> 。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> shuffleManager = instantiateClass[<span class="type">ShuffleManager</span>](shuffleMgrClass)</span><br></pre></td></tr></table></figure>

<h3 id="MemoryManager"><a href="#MemoryManager" class="headerlink" title="MemoryManager"></a>MemoryManager</h3><p>​        一种抽象内存管理器，用于强制执行和存储之间共享内存的方式。在这个上下文下，执行内存是指用于在shuffle，join，sort和aggregation中进行计算的内存，而存储内存是指用于在群集中缓存和传播内部数据的内存。 每个JVM都有一个MemoryManager。</p>
<p>​        UnifiedMemoryManager和StaticMemoryManager继承了MemoryManager这个抽象类，spark1.6+默认使用UnifiedMemoryManager</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> useLegacyMemoryManager = conf.getBoolean(<span class="string">"spark.memory.useLegacyMode"</span>, <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> =</span><br><span class="line">      <span class="keyword">if</span> (useLegacyMemoryManager) &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">StaticMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<h3 id="blockManager"><a href="#blockManager" class="headerlink" title="blockManager"></a>blockManager</h3><p>blockManager运行在每个节点上，包括driver和Executor，提供对本地或远端节点上的内存、磁盘及堆外内存中Block的管理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(executorId, rpcEnv, blockManagerMaster,</span><br><span class="line">      serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,</span><br><span class="line">      blockTransferService, securityManager, numUsableCores)</span><br></pre></td></tr></table></figure>

<h3 id="metricsSystem"><a href="#metricsSystem" class="headerlink" title="metricsSystem"></a>metricsSystem</h3><p>Spark中的master, worker, executor, client driver都可以创建metricsSystem，metricsSytem用来监控Spark组件中的各种指标信息，例如MasterSource, WorkerSource或JvmSource，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> metricsSystem = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      <span class="comment">// Don't start metrics system right now for Driver.</span></span><br><span class="line">      <span class="comment">// We need to wait for the task scheduler to give us an app ID.</span></span><br><span class="line">      <span class="comment">// Then we can start the metrics system.</span></span><br><span class="line">      <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"driver"</span>, conf, securityManager)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We need to set the executor ID before the MetricsSystem is created because sources and</span></span><br><span class="line">      <span class="comment">// sinks specified in the metrics configuration file will want to incorporate this executor's</span></span><br><span class="line">      <span class="comment">// ID into the metrics they report.</span></span><br><span class="line">      conf.set(<span class="string">"spark.executor.id"</span>, executorId)</span><br><span class="line">      <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"executor"</span>, conf, securityManager)</span><br><span class="line">      ms.start()</span><br><span class="line">      ms</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="outputCommitCoordinator"><a href="#outputCommitCoordinator" class="headerlink" title="outputCommitCoordinator"></a>outputCommitCoordinator</h3><p>决定任务是否可以将输出数据提交到HDFS</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> outputCommitCoordinator = mockOutputCommitCoordinator.getOrElse &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">OutputCommitCoordinator</span>(conf, isDriver)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><p>​        SparkContext是Spark application的主要入口，SparkContext代表着应用程序与Spark集群的连接，开发者可以使用SparkContext在集群中创建RDD，累加器，或者广播变量。需要注意的是，在每个JVM中只能有一个正在运行的SparkContext，如果要让新的SparkContext在JVM中运行，必须先停止正在运行的SparkContext。</p>
<p>创建的大致流程图如下</p>
<img src="https://willipic.oss-cn-hangzhou.aliyuncs.com/concurrency/SparkContext%E6%B5%81%E7%A8%8B.001.png" style="zoom:70%;" />

<ul>
<li><p>SparkContext在创建之前，会确认是否允许多个SparkContext同时在JVM中运行，如果Spark允许，那么只生成警告，若不允许则抛出异常</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If true, log warnings instead of throwing exceptions when multiple SparkContexts are active</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> allowMultipleContexts: <span class="type">Boolean</span> =</span><br><span class="line">    config.getBoolean(<span class="string">"spark.driver.allowMultipleContexts"</span>, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In order to prevent multiple SparkContexts from being active at the same time, mark this</span></span><br><span class="line">  <span class="comment">// context as having started construction.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> this must be placed at the beginning of the SparkContext constructor.</span></span><br><span class="line">  <span class="type">SparkContext</span>.markPartiallyConstructed(<span class="keyword">this</span>, allowMultipleContexts)</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来SparkContext中定义了以下这些私有变量，这其中有很多Spark application创建过程中的重要组件，这些变量是只能通过SparkContext中定义的一系列类似java类中的get方法来访问的，但是在运行过程中是不能够改变的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _listenerBus: <span class="type">LiveListenerBus</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _statusTracker: <span class="type">SparkStatusTracker</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _progressBar: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _ui: <span class="type">Option</span>[<span class="type">SparkUI</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _hadoopConfiguration: <span class="type">Configuration</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _executorMemory: <span class="type">Int</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: <span class="type">TaskScheduler</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _heartbeatReceiver: <span class="type">RpcEndpointRef</span> = _</span><br><span class="line"> <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: <span class="type">DAGScheduler</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _applicationId: <span class="type">String</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _applicationAttemptId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _eventLogger: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _executorAllocationManager: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _cleaner: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = <span class="type">None</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _listenerBusStarted: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _jars: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _files: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _shutdownHookRef: <span class="type">AnyRef</span> = _</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _statusStore: <span class="type">AppStatusStore</span> = _</span><br></pre></td></tr></table></figure>
</li>
<li><p>在创建SparkContext的过程中，会检查SparkConf中的配置是否完整，如果缺少某些关键配置，则会抛出异常，并判断是以什么形式提交Spark任务</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!_conf.contains(<span class="string">"spark.master"</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"A master URL must be set in your configuration"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!_conf.contains(<span class="string">"spark.app.name"</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"An application name must be set in your configuration"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// log out spark.app.name in the Spark driver logs</span></span><br><span class="line">    logInfo(<span class="string">s"Submitted application: <span class="subst">$appName</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster</span></span><br><span class="line">    <span class="keyword">if</span> (master == <span class="string">"yarn"</span> &amp;&amp; deployMode == <span class="string">"cluster"</span> &amp;&amp; !_conf.contains(<span class="string">"spark.yarn.app.id"</span>)) 		&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Detected yarn cluster mode, but isn't running on a cluster. "</span> +</span><br><span class="line">        <span class="string">"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.logConf"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      logInfo(<span class="string">"Spark configuration:\n"</span> + _conf.toDebugString)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来SparkContext会对Driver进行一系列配置，部分代码如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Set Spark driver host and port system properties. This explicitly sets the configuration</span></span><br><span class="line">   <span class="comment">// instead of relying on the default value of the config constant.</span></span><br><span class="line">   _conf.set(<span class="type">DRIVER_HOST_ADDRESS</span>, _conf.get(<span class="type">DRIVER_HOST_ADDRESS</span>))</span><br><span class="line">   _conf.setIfMissing(<span class="string">"spark.driver.port"</span>, <span class="string">"0"</span>)</span><br><span class="line">  </span><br><span class="line">   _conf.set(<span class="string">"spark.executor.id"</span>, <span class="type">SparkContext</span>.<span class="type">DRIVER_IDENTIFIER</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// Create the Spark execution environment (cache, map output tracker, etc)</span></span><br><span class="line">   _env = createSparkEnv(_conf, isLocal, listenerBus)</span><br><span class="line">   <span class="type">SparkEnv</span>.set(_env)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在创建TaskScheduler之前创建Sparkui，这样能让Sparkui更好监听Spark任务中的各项指标</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">_ui =</span><br><span class="line">      <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.ui.enabled"</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="type">Some</span>(<span class="type">SparkUI</span>.create(<span class="type">Some</span>(<span class="keyword">this</span>), _statusStore, _conf, _env.securityManager, appName, <span class="string">""</span>,</span><br><span class="line">          startTime))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// For tests, do not enable the UI</span></span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// Bind the UI before starting the task scheduler to communicate</span></span><br><span class="line">    <span class="comment">// the bound port to the cluster manager properly</span></span><br><span class="line">    _ui.foreach(_.bind())</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来对Executor的内存大小进行配置，并根据配置创建ExecutorEnv</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">_executorMemory = _conf.getOption(<span class="string">"spark.executor.memory"</span>)</span><br><span class="line">      .orElse(<span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_EXECUTOR_MEMORY"</span>)))</span><br><span class="line">      .orElse(<span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_MEM"</span>))</span><br><span class="line">      .map(warnSparkMem))</span><br><span class="line">      .map(<span class="type">Utils</span>.memoryStringToMb)</span><br><span class="line">      .getOrElse(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Convert java options to env vars as a work around</span></span><br><span class="line">    <span class="comment">// since we can't set env vars directly in sbt.</span></span><br><span class="line">    <span class="keyword">for</span> &#123; (envKey, propKey) &lt;- <span class="type">Seq</span>((<span class="string">"SPARK_TESTING"</span>, <span class="string">"spark.testing"</span>))</span><br><span class="line">      value &lt;- <span class="type">Option</span>(<span class="type">System</span>.getenv(envKey)).orElse(<span class="type">Option</span>(<span class="type">System</span>.getProperty(propKey)))&#125; &#123;</span><br><span class="line">      executorEnvs(envKey) = value</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_PREPEND_CLASSES"</span>)).foreach &#123; v =&gt;</span><br><span class="line">      executorEnvs(<span class="string">"SPARK_PREPEND_CLASSES"</span>) = v</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// The Mesos scheduler backend relies on this environment variable to set executor memory.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Set this only in the Mesos scheduler.</span></span><br><span class="line">    executorEnvs(<span class="string">"SPARK_EXECUTOR_MEMORY"</span>) = executorMemory + <span class="string">"m"</span></span><br><span class="line">    executorEnvs ++= _conf.getExecutorEnv</span><br><span class="line">    executorEnvs(<span class="string">"SPARK_USER"</span>) = sparkUser</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建心跳检测器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// We need to register "HeartbeatReceiver" before "createTaskScheduler" because Executor will</span></span><br><span class="line">    <span class="comment">// retrieve "HeartbeatReceiver" in the constructor. (SPARK-6640)</span></span><br><span class="line">    _heartbeatReceiver = env.rpcEnv.setupEndpoint(</span><br><span class="line">      <span class="type">HeartbeatReceiver</span>.<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">HeartbeatReceiver</span>(<span class="keyword">this</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建DAGScheduler和TaskScheduler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create and start the scheduler</span></span><br><span class="line">    <span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">    _schedulerBackend = sched</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">    _dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</span><br><span class="line">    _heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动TaskScheduler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's</span></span><br><span class="line">    <span class="comment">// constructor</span></span><br><span class="line">    _taskScheduler.start()</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动metricSystem给Spark Application id 赋值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The metrics system for Driver need to be set spark.app.id to app ID.</span></span><br><span class="line">    <span class="comment">// So it should start after we get app ID from the task scheduler and set spark.app.id.</span></span><br><span class="line">    _env.metricsSystem.start()</span><br><span class="line">    <span class="comment">// Attach the driver metrics servlet handler to the web ui after the metrics system is started.</span></span><br><span class="line">    _env.metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler)))</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用一下三个方法，创建SparkContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setupAndStartListenerBus()</span><br><span class="line">  postEnvironmentUpdate()</span><br><span class="line">  postApplicationStart()</span><br></pre></td></tr></table></figure>







</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Willi
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://thisiswilli.cn/2020/03/16/SparkCore%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8BSparkContext-SparkConf-SparkEnv/" title="SparkCore源码阅读之SparkContext,SparkConf,SparkEnv">https://thisiswilli.cn/2020/03/16/SparkCore源码阅读之SparkContext-SparkConf-SparkEnv/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

      <div>
      
        
      
      </div>

      <div>
        
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">
            -------------本文结束
            <i class="fa fa-paw"></i>
            感谢您的阅读-------------
        </div>
    
</div>

        
      </div>
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/big-data/" rel="tag"># big data</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/14/Java%E4%B8%AD%E5%8F%98%E9%87%8F%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E5%88%86%E6%9E%90/" rel="prev" title="Java中变量的线程安全分析">
      <i class="fa fa-chevron-left"></i> Java中变量的线程安全分析
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/19/%E4%BB%8E%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%92%8CJavaAPI%E5%B1%82%E9%9D%A2%E8%A7%A3%E9%87%8A%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81/" rel="next" title="从操作系统和JavaAPI层面解释线程的状态">
      从操作系统和JavaAPI层面解释线程的状态 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkCore之SparkContext-SparkConf"><span class="nav-number">1.</span> <span class="nav-text">SparkCore之SparkContext,SparkConf</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark程序入口"><span class="nav-number">1.1.</span> <span class="nav-text">Spark程序入口</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkConf"><span class="nav-number">1.2.</span> <span class="nav-text">SparkConf</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkEnv"><span class="nav-number">1.3.</span> <span class="nav-text">SparkEnv</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安全管理器SecurityManager"><span class="nav-number">1.3.1.</span> <span class="nav-text">安全管理器SecurityManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPC通信环境RpcEnv"><span class="nav-number">1.3.2.</span> <span class="nav-text">RPC通信环境RpcEnv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#序列化管理器serializerManager"><span class="nav-number">1.3.3.</span> <span class="nav-text">序列化管理器serializerManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#闭包序列化器closureSerializer"><span class="nav-number">1.3.4.</span> <span class="nav-text">闭包序列化器closureSerializer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广播管理器broadcastManager"><span class="nav-number">1.3.5.</span> <span class="nav-text">广播管理器broadcastManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleManager"><span class="nav-number">1.3.6.</span> <span class="nav-text">ShuffleManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MemoryManager"><span class="nav-number">1.3.7.</span> <span class="nav-text">MemoryManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#blockManager"><span class="nav-number">1.3.8.</span> <span class="nav-text">blockManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#metricsSystem"><span class="nav-number">1.3.9.</span> <span class="nav-text">metricsSystem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#outputCommitCoordinator"><span class="nav-number">1.3.10.</span> <span class="nav-text">outputCommitCoordinator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkContext"><span class="nav-number">1.4.</span> <span class="nav-text">SparkContext</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Willi"
      src="/images/xiaolang.jpeg">
  <p class="site-author-name" itemprop="name">Willi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ThisisWilli" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ThisisWilli" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:295258774@qq.com" title="E-Mail → mailto:295258774@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/wever42/" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;wever42&#x2F;" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Jerseywwwwei" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;Jerseywwwwei" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i>CSDN</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Willi</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">127k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:55</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      &nbsp;本站访客数&nbsp;<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人</span>	
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      &nbsp;本站总访问量&nbsp;<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span>	
    </span>
  
</div>









      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'WpktFBGS1tJFUonSWplyupKR-gzGzoHsz',
      appKey     : '0AnoRoyQ9ljCCexp2J2QmvBn',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  <div class="bg_content">
  <canvas id="canvas"></canvas>
</div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>